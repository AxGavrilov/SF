{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n",
    "# Predict TripAdvisor Rating\n",
    "## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n",
    "**По ходу задачи:**\n",
    "* Прокачаем работу с pandas\n",
    "* Научимся работать с Kaggle Notebooks\n",
    "* Поймем как делать предобработку различных данных\n",
    "* Научимся работать с пропущенными данными (Nan)\n",
    "* Познакомимся с различными видами кодирования признаков\n",
    "* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n",
    "* И совсем немного затронем ML\n",
    "* И многое другое...   \n",
    "\n",
    "\n",
    "\n",
    "### И самое важное, все это вы сможете сделать самостоятельно!\n",
    "\n",
    "*Этот Ноутбук являетсся Примером/Шаблоном к этому соревнованию (Baseline) и не служит готовым решением!*   \n",
    "Вы можете использовать его как основу для построения своего решения.\n",
    "\n",
    "> что такое baseline решение, зачем оно нужно и почему предоставлять baseline к соревнованию стало важным стандартом на kaggle и других площадках.   \n",
    "**baseline** создается больше как шаблон, где можно посмотреть как происходит обращение с входящими данными и что нужно получить на выходе. При этом МЛ начинка может быть достаточно простой, просто для примера. Это помогает быстрее приступить к самому МЛ, а не тратить ценное время на чисто инженерные задачи. \n",
    "Также baseline являеться хорошей опорной точкой по метрике. Если твое решение хуже baseline - ты явно делаешь что-то не то и стоит попробовать другой путь) \n",
    "\n",
    "В контексте нашего соревнования baseline идет с небольшими примерами того, что можно делать с данными, и с инструкцией, что делать дальше, чтобы улучшить результат.  Вообще готовым решением это сложно назвать, так как используются всего 2 самых простых признака (а остальные исключаются)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "# Загружаем специальный удобный инструмент для разделения датасета:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загружаем пакет для работы со временем:\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Загружаем модель для работы с регулярными выражениями для чистки данных\n",
    "import re\n",
    "\n",
    "# Загружаем модуль collections для подсчета частоты встречаемости уникальных значений\n",
    "import collections\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\n",
    "df_train = pd.read_csv(DATA_DIR+'/main_task.csv')\n",
    "df_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\n",
    "sample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('main_task.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее по признакам:\n",
    "* City: Город \n",
    "* Cuisine Style: Кухня\n",
    "* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n",
    "* Price Range: Цены в ресторане в 3 категориях\n",
    "* Number of Reviews: Количество отзывов\n",
    "* Reviews: 2 последних отзыва и даты этих отзывов\n",
    "* URL_TA: страница ресторана на 'www.tripadvisor.com' \n",
    "* ID_TA: ID ресторана в TripAdvisor\n",
    "* Rating: Рейтинг ресторана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузим и посмотрим на весь датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.info())\n",
    "display(data.describe())\n",
    "display(data.describe(include = ['object']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, большинство признаков у нас требует очистки и предварительной обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приведем наименование некоторых столбцов к более удобному виду, уберем пробелы\n",
    "data = data.rename(columns={'Cuisine Style': 'Cuisine_Style','Price Range': 'Price_Range', 'Number of Reviews': \n",
    "                            'Number_of_Reviews'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рассмотрим все 10 столбцов по отдельности, выполним очистку и преобразование данных, выполним EDA, создадим новые признаки для модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant_id — идентификационный номер ресторана / сети ресторанов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Restaurant_id']].info()\n",
    "display(pd.DataFrame(data.Restaurant_id.value_counts()))\n",
    "data.Restaurant_id.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### нет пропусков, категориальный признак, ID повторяются достаточно часто, предположим, что это сети ресторанов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем соотвествующий признак\n",
    "# \"0\" - Restaurant_id - уникальный\n",
    "# \"1\" - Restaurant_id повторяется (предположим, это сеть)\n",
    "chain = data.Restaurant_id.value_counts()[data.Restaurant_id.value_counts()>1].index.tolist()\n",
    "data['Сhain'] = data[data.Restaurant_id.isin(chain)].Restaurant_id.apply(lambda x: 1)\n",
    "data.Сhain = data['Сhain'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Сhain   40000 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 312.6 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Сhain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>35295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Сhain\n",
       "1.0  35295\n",
       "0.0   4705"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    40000.000000\n",
       "mean         0.882375\n",
       "std          0.322168\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: Сhain, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# познакомимся с новым столбцом\n",
    "data.loc[:, ['Сhain']].info()\n",
    "display(pd.DataFrame(data.Сhain.value_counts()))\n",
    "data.Сhain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City — город, в котором находится ресторан"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['City']].info()\n",
    "display(pd.DataFrame(data.City.value_counts()))\n",
    "data.City.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### нет пропусков, категориальный признак, всего 31 город, преобразуем методом get_dummies, подход - One-Hot Encoding, исходную колонку пока сохраним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим дамми переменные для каждого города\n",
    "data['City_gd'] = data['City']\n",
    "data = pd.get_dummies(data, columns=[ 'City_gd',], dummy_na=True)\n",
    "# так как пропущенных значений нет столбец City_gd_nan удалим\n",
    "data = data.drop(['City_gd_nan'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признак 'City_Num_rest',который содержит информацию о количестве заведений в городе\n",
    "\n",
    "cities1 = {'London':5757,\n",
    "'Paris':4897,\n",
    "'Madrid':3108,\n",
    "'Barcelona':2734,\n",
    "'Berlin':2155,\n",
    "'Milan':2133,\n",
    "'Rome':2078,\n",
    "'Prague':1443,\n",
    "'Lisbon':1300,\n",
    "'Vienna':1166,\n",
    "'Amsterdam':1086,\n",
    "'Brussels':1060,\n",
    "'Hamburg':949,\n",
    "'Munich':893,\n",
    "'Lyon':892,\n",
    "'Stockholm':820,\n",
    "'Budapest':816,\n",
    "'Warsaw':727,\n",
    "'Dublin':673,\n",
    "'Copenhagen':659,\n",
    "'Athens':628,\n",
    "'Edinburgh':596,\n",
    "'Zurich':538,\n",
    "'Oporto':513,\n",
    "'Geneva':481,\n",
    "'Krakow':443,\n",
    "'Oslo':385,\n",
    "'Helsinki':376,\n",
    "'Bratislava':301,\n",
    "'Luxembourg':210,\n",
    "'Ljubljana':183}\n",
    "\n",
    "data['City_Num_rest'] = data['City'].replace(cities1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признак 'City_Population', который содержит информацию о численности населения города и агломерации\n",
    "# (взято с wiki)\n",
    "\n",
    "cities2 = {'London':8982000,\n",
    "'Paris':2196936,\n",
    "'Madrid':6642000,\n",
    "'Barcelona':5575000,\n",
    "'Berlin':3769000,\n",
    "'Milan':1352000,\n",
    "'Rome':2873000,\n",
    "'Prague':1309000,\n",
    "'Lisbon':504718,\n",
    "'Vienna':1897000,\n",
    "'Amsterdam':821752,\n",
    "'Brussels':174383,\n",
    "'Hamburg':1845000,\n",
    "'Munich':1472000,\n",
    "'Lyon':516092,\n",
    "'Stockholm':975904,\n",
    "'Budapest':1752000,\n",
    "'Warsaw':1708000,\n",
    "'Dublin':1388000,\n",
    "'Copenhagen':602481,\n",
    "'Athens':664046,\n",
    "'Edinburgh':482005,\n",
    "'Zurich':402762,\n",
    "'Oporto':214349,\n",
    "'Geneva':499480,\n",
    "'Krakow':779115,\n",
    "'Oslo':681067,\n",
    "'Helsinki':631695,\n",
    "'Bratislava':424428,\n",
    "'Luxembourg':613894,\n",
    "'Ljubljana':279631}\n",
    "\n",
    "data['City_Population'] = data['City'].replace(cities2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признак 'City_Turists',который содержит информацию о количестве туристов посетивших город по данным Mastercard\n",
    "# основной источник: https://en.wikipedia.org/wiki/List_of_cities_by_international_visitors\n",
    "\n",
    "cities3 = {'Luxembourg':3000000, \n",
    "'Zurich':2240000, \n",
    "'Geneva':1150000, \n",
    "'Dublin':4970000, \n",
    "'Oslo':4868681, \n",
    "'Amsterdam':8000000, \n",
    "'Copenhagen':1630000, \n",
    "'Stockholm':2080000, \n",
    "'Vienna':6690000, \n",
    "'Helsinki':4000000, \n",
    "'Berlin':4940000, \n",
    "'Hamburg':1450000, \n",
    "'Munich':5250000, \n",
    "'Brussels':2710000, \n",
    "'London':19880000, \n",
    "'Paris':18030000, \n",
    "'Lyon':4868539, \n",
    "'Milan':7650000, \n",
    "'Rome':7120000, \n",
    "'Madrid':5260000, \n",
    "'Barcelona':8200000, \n",
    "'Ljubljana':841320, \n",
    "'Prague':5810000, \n",
    "'Lisbon':3630000, \n",
    "'Oporto':4868539, \n",
    "'Athens':2680000, \n",
    "'Bratislava':1400000, \n",
    "'Budapest':3360000, \n",
    "'Warsaw':1370000, \n",
    "'Krakow':1313277, \n",
    "'Edinburgh':1660000}\n",
    "\n",
    "data['City_Turists'] = data['City'].replace(cities3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признаки 'Rest_on_Pop' и 'Rest_on_Turists',\n",
    "# которые содержыт информацию о количестве ресторанов на одного жителя и туриста\n",
    "\n",
    "data['Rest_on_Pop'] = data['City_Num_rest']/data['City_Population']\n",
    "data['Rest_on_Turists'] = data['City_Num_rest']/data['City_Turists']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuisine_Style — кухня или кухни, к которым можно отнести блюда, предлагаемые в ресторане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Cuisine_Style']].info()\n",
    "display(pd.DataFrame(data.Cuisine_Style.value_counts()))\n",
    "data.Cuisine_Style.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### есть пропуски, категориальный признак, необходима обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признак 'Number_of_Reviews_isNAN',который содержит инфо о пропусках\n",
    "data['Cuisine_Style_isNAN'] = pd.isna(data['Cuisine_Style']).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# судя по формату данных необходимо выполнить очистку от [], '',\n",
    "# приведем к одному формату (lower, без пробелов, то есть , одна кухня - одно слово),\n",
    "# разделим виды кухонь, разделитель ',' и преобразуем в список\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: str(x))\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\"[\",\"\"))\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\"]\",\"\"))\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\"'\",\"\"))\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.lower())\n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словарь ключь - кухня, значение - количество ресторанов с этой кухней\n",
    "cuisine_exploded = data.explode('Cuisine_Style')\n",
    "cousins = dict(cuisine_exploded['Cuisine_Style'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки кухонь, по частоте встречаемости\n",
    "all_hight_1000 = []\n",
    "top_top = []\n",
    "top_mid = []\n",
    "top_low = []\n",
    "\n",
    "for i, j in cousins.items():\n",
    "    if j > 10000:\n",
    "        all_hight_1000.append(i)\n",
    "        top_top.append(i)\n",
    "    elif j < 1000:\n",
    "        top_low.append(i)\n",
    "    else:\n",
    "        all_hight_1000.append(i)\n",
    "        top_mid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем бинарную переменную наличия у ресторана кухни, с частотой встречаемости более 1 000 \n",
    "for i in all_hight_1000:\n",
    "    fin = []\n",
    "    for j in range(len(data['Cuisine_Style'])):\n",
    "        if i in data['Cuisine_Style'][j]:\n",
    "            fin.append(1)\n",
    "        else:\n",
    "            fin.append(0)\n",
    "    data['More_1000'] = fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем бинарную переменную наличия у ресторана кухни, с частотой встречаемости более 10 000 \n",
    "for i in top_top:\n",
    "    fin = []\n",
    "    for j in range(len(data['Cuisine_Style'])):\n",
    "        if i in data['Cuisine_Style'][j]:\n",
    "            fin.append(1)\n",
    "        else:\n",
    "            fin.append(0)\n",
    "    data['Top_top'] = fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Создаем бинарную переменную наличия у ресторана кухни, с частотой встречаемости более 1 000 \n",
    "    for i in all_hight_1000:\n",
    "        fin = []\n",
    "        for j in range(len(data['Cuisine_Style'])):\n",
    "            if i in data['Cuisine_Style'][j]:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        data['More_1000'] = fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuisine_Style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vegetarianfriendly</th>\n",
       "      <td>20472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>european</th>\n",
       "      <td>10060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediterranean</th>\n",
       "      <td>6277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian</th>\n",
       "      <td>5964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veganoptions</th>\n",
       "      <td>4486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salvadoran</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latvian</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yunnan</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burmese</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xinjiang</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Cuisine_Style\n",
       "vegetarianfriendly          20472\n",
       "european                    10060\n",
       "mediterranean                6277\n",
       "italian                      5964\n",
       "veganoptions                 4486\n",
       "...                           ...\n",
       "salvadoran                      1\n",
       "latvian                         1\n",
       "yunnan                          1\n",
       "burmese                         1\n",
       "xinjiang                        1\n",
       "\n",
       "[125 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим наиболее часто встречающееся значение и заполним им пропуски\n",
    "# вынесено в предобработку столбца\n",
    "pd.DataFrame(cuisine_exploded.Cuisine_Style.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним пропуски наиболее часто встречающимся видом кухни (вынесено в предобработку столбца, \n",
    "data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: 'vegetarianfriendly' if x[0] == 'nan' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим новый признак - количество предлагаемых в ресторане кухонь\n",
    "data['Cuisines'] = data['Cuisine_Style'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поскольку средние значение количества кухонь на ресторан значительно различаются по городам\n",
    "# отнормируем количество предлагаемых кухонь по среднему значению по городу\n",
    "data['Cuisines_norm'] = data['Cuisines']/data.City.map(dict(data.groupby(['City'])['Cuisines'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking — место, которое занимает данный ресторан среди всех ресторанов своего города  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Ranking']].info()\n",
    "display(pd.DataFrame(data.Ranking.value_counts()))\n",
    "data.Ranking.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### пропусков нет, числовой признак, обратим внимание, что это место среди ресторанов своего города, необходимо рассматривать вместе с City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим распределение признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "df_train['Ranking'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['City'].value_counts(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на топ 10 городов\n",
    "for x in (df_train['City'].value_counts())[0:10].index:\n",
    "    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим распределение целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Rating'].value_counts(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим распределение целевой переменной относительно признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормируем Ranking по среднему значению для города\n",
    "data['Ranking_norm'] = data['Ranking']/data.City.map(dict(data.groupby(['City'])['Ranking'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Rating — рейтинг ресторана по данным TripAdvisor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Rating']].info()\n",
    "display(pd.DataFrame(data.Rating.value_counts()))\n",
    "data.Rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### пропусков нет, числовой признак,именно это значение должна будет предсказывать модель, возможные значения от 1 до 5, шаг 0,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price_Range — диапазон цен в ресторане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Price_Range']].info()\n",
    "print (pd.DataFrame(data.Price_Range.value_counts())) #display отображает не информативно\n",
    "data.Price_Range.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### есть пропуски, ординальный признак, необходима обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признак, который содержит инфо о пропусках\n",
    "data['Price_Range_isNAN'] = pd.isna(data['Price_Range']).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим словарь для преобразования значений уровня цен\n",
    "price_dict = {'$': 1, '$$ - $$$': 2, '$$$$': 3}\n",
    "    \n",
    "# выполним преобразование\n",
    "data['Price_Range'].replace(price_dict, inplace=True)\n",
    "    \n",
    "# заполним пропущенные значения наиболее часто встречающимся\n",
    "data['Price_Range'].fillna(data.City.map(dict(data.groupby(['City'])['Price_Range'].mean())), inplace=True)\n",
    "# Поскольку заполняли средним, то необходимо округлить значение\n",
    "data['Price_Range'] = data['Price_Range'].apply(round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Number_of_Reviews — количество отзывов о ресторане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим наличие пропусков\n",
    "data['Number_of_Reviews'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем признак 'Number_of_Reviews_isNAN',который содержит инфо о пропусках\n",
    "data['Number_of_Reviews_isNAN'] = pd.isna(data['Number_of_Reviews']).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 1 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Number_of_Reviews  40000 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 312.6 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number_of_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125.0</th>\n",
       "      <td>2593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Number_of_Reviews\n",
       "125.0                2593\n",
       "2.0                  1916\n",
       "3.0                  1636\n",
       "4.0                  1370\n",
       "5.0                  1181\n",
       "...                   ...\n",
       "1691.0                  1\n",
       "1186.0                  1\n",
       "738.0                   1\n",
       "1721.0                  1\n",
       "1799.0                  1\n",
       "\n",
       "[1459 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    40000.000000\n",
       "mean       124.836575\n",
       "std        286.113295\n",
       "min          2.000000\n",
       "25%         10.000000\n",
       "50%         38.000000\n",
       "75%        125.000000\n",
       "max       9660.000000\n",
       "Name: Number_of_Reviews, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, ['Number_of_Reviews']].info()\n",
    "display(pd.DataFrame(data.Number_of_Reviews.value_counts()))\n",
    "data.Number_of_Reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполним пропуски наиболее часто встречающимся значением\n",
    "data['Number_of_Reviews'].fillna(data.City.map(dict(data.groupby(['City'])['Number_of_Reviews'].mean())), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим признак с информацией о среднем числе отзывов на одного туриста\n",
    "data['Reviews_per_turist'] = data['Number_of_Reviews']/data['City_Turists']\n",
    "    \n",
    "# Поскольку среднее количество отзывов различается по городам\n",
    "# отнормируем количество отзывов по средне-городскому значению\n",
    "data['Number_of_Reviews_norm'] = data['Number_of_Reviews']/data.City.map(dict(data.groupby(['City'])['Number_of_Reviews']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Reviews — данные о двух отзывах, которые отображаются на сайте ресторана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Reviews']].info()\n",
    "display(pd.DataFrame(data.Reviews.value_counts()))\n",
    "data.Reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим переменные содержащие даты первого и второго отзывов\n",
    "pattern = re.compile('\\d\\d/\\d\\d/\\d\\d\\d\\d')\n",
    "data['Reviews'] = data['Reviews'].fillna('')\n",
    "data['Review_dates'] = data['Reviews'].apply(lambda x: pattern.findall(x))\n",
    "data['Review_1'] = data['Review_dates'].apply(lambda x: x[:1])\n",
    "data['Review_2'] = data['Review_dates'].apply(lambda x: x[1:])\n",
    "\n",
    "\n",
    "# заполняем пропуски в датах самым частым значением для каждого столбца ревью:\n",
    "for i in range(len(data['Review_1'])):\n",
    "    if len(data['Review_1'][i]) == 0:\n",
    "        data['Review_1'][i].append('01/07/2018')\n",
    "for i in range(len(data['Review_2'])):\n",
    "    if len(data['Review_2'][i]) == 0:\n",
    "        data['Review_2'][i].append('01/03/2018')\n",
    "\n",
    "\n",
    "# переводим в формат Datetime:\n",
    "data['Review_1'] = data['Review_1'].apply(lambda x: datetime.strptime(x[0], '%m/%d/%Y'))\n",
    "data['Review_2'] = data['Review_2'].apply(lambda x: datetime.strptime(x[0], '%m/%d/%Y'))\n",
    "data.drop(['Review_dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистим переменную Reviews, сделаем чистые словари для анализа повторных отзывов\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"\\\\\",\"\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"\\\"\",\"'\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\",\",\", \"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"',  '\",\",\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"],  [\",\",\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\",  \",\" \"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\", \",\" \"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"['\",\"\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"']\",\"\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"[nan \",\"nan,\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"[\",\"\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"]\",\"\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"'\",\"\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\" nan\",\",nan\"))\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.lower())\n",
    "data['Reviews'] = data['Reviews'].apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем новый признак 'Review_len' со средний длинной двух отзывов\n",
    "data['Reviews_len'] = data['Reviews'].apply(lambda x: ((len(x[0])+len(x[1]))/2) if len(x) == 4 else len(x[0]))\n",
    "# Нормируем по среднему по городу\n",
    "data['Reviews_len_norm'] = data['Reviews_len']/data.City.map(dict(data.groupby(['City'])['Reviews_len'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполним пропуски средним значением\n",
    "data['Reviews_len'].replace(to_replace=0, value=data['Reviews_len'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем новый признак 'Review_Delta' с разницей в днях между двумя отзывыами из массива:\n",
    "data['Review_Delta'] = data['Review_1'] - data['Review_2']\n",
    "data['Review_Delta'] = data['Review_Delta'].apply(lambda x: x.days)\n",
    "\n",
    "# создаем новый признак 'Review_Delta_now' с разницей в днях между последним отзывом и сегодняшней датой:\n",
    "data['Review_Delta_now'] = datetime.strptime('02/23/2021', '%m/%d/%Y') - data['Review_1'] \n",
    "data['Review_Delta_now'] =  data['Review_Delta_now'].apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Поскольку некоторые отзывы полностью повторяются\n",
    "    # cоздадим признак Reviews_repit, отражающий наличие повторных отзывов среди имеющихся у ресторана\n",
    "    #(повторы на уровне всей базы)\n",
    "\n",
    "    c = collections.Counter()\n",
    "\n",
    "    for i in range(len(data['Reviews'])):\n",
    "        if len(data['Reviews'][i]) == 4:\n",
    "            c[data['Reviews'][i][0]] += 1\n",
    "            c[data['Reviews'][i][1]] += 1\n",
    "        if len(data['Reviews'][i]) == 2 and len(data['Reviews'][i][0]) > 0:\n",
    "            c[data['Reviews'][i][0]] += 1\n",
    "\n",
    "    fin = []\n",
    "\n",
    "    for i in range(len(data['Reviews'])):\n",
    "        if len(data['Reviews'][i]) == 4:\n",
    "            if c[data['Reviews'][i][0]] > 1 or c[data['Reviews'][i][1]] > 1:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        elif len(data['Reviews'][i]) == 2 and len(data['Reviews'][i][0]) > 0:\n",
    "            if c[data['Reviews'][i][0]] > 1:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        else:\n",
    "            fin.append(0)\n",
    "\n",
    "    data['Reviews_repit'] = fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### есть пропуски, категориальный признак, есть повторояющиеся отзывы, возможно использование для дальнейшего улучшения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL_TA — URL страницы ресторана на TripAdvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['URL_TA']].info()\n",
    "display(pd.DataFrame(data.URL_TA.value_counts()))\n",
    "data.URL_TA.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### нет пропусков, категориальный признак, есть повторяющиеся значения, непонятно влияние этого параметра на рейтинг, в текущий момент не используем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID_TA — идентификатор ресторана в базе данных TripAdvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['ID_TA']].info()\n",
    "display(pd.DataFrame(data.ID_TA.value_counts()))\n",
    "data.ID_TA.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### нет пропусков, категориальный признак, есть повторяющиеся значения, непонятно влияние этого параметра на рейтинг, в текущий момент не используем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корреляция признаков\n",
    "посмотрим, как признаки связаны между собой и с целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sample'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4c45002cfa96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3997\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3998\u001b[0m         )\n\u001b[0;32m   3999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3970\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['sample'] not found in axis\""
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "sns.heatmap(data.drop(['sample'], axis=1).corr(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сильная положительная корреляция между кол-вом кухонь в ресторане и рейтингом внутри одного города"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Теперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# на всякий случай, заново подгружаем данные\n",
    "df_train = pd.read_csv(DATA_DIR+'/main_task.csv')\n",
    "df_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_data(df_input):\n",
    "    '''includes several functions to pre-process the predictor data.'''\n",
    "    \n",
    "    data = df_input.copy()\n",
    "    \n",
    "    # ################### 1. Предобработка ############################################################## \n",
    "    # убираем ненужные для модели признаки\n",
    "    data.drop(['ID_TA',], axis = 1, inplace=True)\n",
    "    # приведем наименование некоторых столбцов к более удобному виду, уберем пробелы\n",
    "    data = data.rename(columns={'Cuisine Style': 'Cuisine_Style','Price Range': 'Price_Range', 'Number of Reviews': \n",
    "                                'Number_of_Reviews'})\n",
    "    \n",
    "    # ################### 2. NAN ############################################################## \n",
    "    # Обрабатываем пропуски отдельно для каждой переменной\n",
    "    \n",
    "    \n",
    "    # ################### 3. Encoding ############################################################## \n",
    "    \n",
    "    # ##########      3.1 City      ##########\n",
    "    \n",
    "    # Создаем дихотомические переменные для каждого города функцией get_dummies\n",
    "    data['City_gd'] = data['City']\n",
    "    data = pd.get_dummies(data, columns=['City_gd'], dummy_na=True)\n",
    "    # так как пропущенных значений нет столбец City_gd_nan удалим\n",
    "    data = data.drop(['City_gd_nan'], axis = 1)\n",
    "\n",
    "    # ##########      3.2 Rewie_date      ##########\n",
    "    \n",
    "    # Создадим переменные содержащие даты первого и второго отзывов\n",
    "    pattern = re.compile('\\d\\d/\\d\\d/\\d\\d\\d\\d')\n",
    "    data['Reviews'] = data['Reviews'].fillna('')\n",
    "    data['Review_dates'] = data['Reviews'].apply(lambda x: pattern.findall(x))\n",
    "    data['Review_1'] = data['Review_dates'].apply(lambda x: x[:1])\n",
    "    data['Review_2'] = data['Review_dates'].apply(lambda x: x[1:])\n",
    "\n",
    "    # заполняем пропуски в датах самым частым значением для каждого столбца ревью:\n",
    "    for i in range(len(data['Review_1'])):\n",
    "        if len(data['Review_1'][i]) == 0:\n",
    "            data['Review_1'][i].append('01/07/2018')\n",
    "    for i in range(len(data['Review_2'])):\n",
    "        if len(data['Review_2'][i]) == 0:\n",
    "            data['Review_2'][i].append('01/03/2018')\n",
    "\n",
    "    # переводим в формат Datetime:\n",
    "    data['Review_1'] = data['Review_1'].apply(\n",
    "                       lambda x: datetime.strptime(x[0], '%m/%d/%Y'))\n",
    "    data['Review_2'] = data['Review_2'].apply(\n",
    "                       lambda x: datetime.strptime(x[0], '%m/%d/%Y'))\n",
    "    data.drop(['Review_dates'], axis=1, inplace=True)    \n",
    "\n",
    "    \n",
    "    # ##########      3.3 Rewie_text      ##########\n",
    "    \n",
    "    # Очистим переменную Reviews, сделаем чистые словари для анализа повторных отзывов\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"\\\\\",\"\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"\\\"\",\"'\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\",\",\", \"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"',  '\",\",\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"],  [\",\",\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\",  \",\" \"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\", \",\" \"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"['\",\"\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"']\",\"\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"[nan \",\"nan,\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"[\",\"\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"]\",\"\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\"'\",\"\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.replace(\" nan\",\",nan\"))\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.lower())\n",
    "    data['Reviews'] = data['Reviews'].apply(lambda x: x.split(\",\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ################### 4. Feature Engineering ####################################################\n",
    "\n",
    "    # ##########      4.1 Restaurant_id      ##########\n",
    "\n",
    "    # Создаем признак уникальности ресторана: \"0\" - уникальный, \"1\" - повторяется (предположим, это сеть)\n",
    "    chain = data.Restaurant_id.value_counts()[data.Restaurant_id.value_counts()>1].index.tolist()\n",
    "    data['Сhain'] = data[data.Restaurant_id.isin(chain)].Restaurant_id.apply(lambda x: 1)\n",
    "    data.Сhain = data['Сhain'].fillna(0)\n",
    "\n",
    "\n",
    "    # ##########      4.2 City      ########## \n",
    "\n",
    "    # создаем признак 'City_Num_rest',который содержит информацию о количестве заведений в городе\n",
    "\n",
    "    cities1 = {'London':5757,\n",
    "    'Paris':4897,\n",
    "    'Madrid':3108,\n",
    "    'Barcelona':2734,\n",
    "    'Berlin':2155,\n",
    "    'Milan':2133,\n",
    "    'Rome':2078,\n",
    "    'Prague':1443,\n",
    "    'Lisbon':1300,\n",
    "    'Vienna':1166,\n",
    "    'Amsterdam':1086,\n",
    "    'Brussels':1060,\n",
    "    'Hamburg':949,\n",
    "    'Munich':893,\n",
    "    'Lyon':892,\n",
    "    'Stockholm':820,\n",
    "    'Budapest':816,\n",
    "    'Warsaw':727,\n",
    "    'Dublin':673,\n",
    "    'Copenhagen':659,\n",
    "    'Athens':628,\n",
    "    'Edinburgh':596,\n",
    "    'Zurich':538,\n",
    "    'Oporto':513,\n",
    "    'Geneva':481,\n",
    "    'Krakow':443,\n",
    "    'Oslo':385,\n",
    "    'Helsinki':376,\n",
    "    'Bratislava':301,\n",
    "    'Luxembourg':210,\n",
    "    'Ljubljana':183}\n",
    "\n",
    "    data['City_Num_rest'] = data['City'].replace(cities1)\n",
    "\n",
    "\n",
    "    # создаем признак 'City_Population', который содержит информацию о численности населения города и агломерации\n",
    "    # (взято с wiki)\n",
    "\n",
    "    cities2 = {'London':8982000,\n",
    "    'Paris':2196936,\n",
    "    'Madrid':6642000,\n",
    "    'Barcelona':5575000,\n",
    "    'Berlin':3769000,\n",
    "    'Milan':1352000,\n",
    "    'Rome':2873000,\n",
    "    'Prague':1309000,\n",
    "    'Lisbon':504718,\n",
    "    'Vienna':1897000,\n",
    "    'Amsterdam':821752,\n",
    "    'Brussels':174383,\n",
    "    'Hamburg':1845000,\n",
    "    'Munich':1472000,\n",
    "    'Lyon':516092,\n",
    "    'Stockholm':975904,\n",
    "    'Budapest':1752000,\n",
    "    'Warsaw':1708000,\n",
    "    'Dublin':1388000,\n",
    "    'Copenhagen':602481,\n",
    "    'Athens':664046,\n",
    "    'Edinburgh':482005,\n",
    "    'Zurich':402762,\n",
    "    'Oporto':214349,\n",
    "    'Geneva':499480,\n",
    "    'Krakow':779115,\n",
    "    'Oslo':681067,\n",
    "    'Helsinki':631695,\n",
    "    'Bratislava':424428,\n",
    "    'Luxembourg':613894,\n",
    "    'Ljubljana':279631}\n",
    "\n",
    "    data['City_Population'] = data['City'].replace(cities2)\n",
    "\n",
    "\n",
    "    # создаем признак 'City_Turists',который содержит информацию о количестве туристов посетивших город по данным Mastercard\n",
    "    # основной источник: https://en.wikipedia.org/wiki/List_of_cities_by_international_visitors\n",
    "\n",
    "    cities3 = {'Luxembourg':3000000, \n",
    "    'Zurich':2240000, \n",
    "    'Geneva':1150000, \n",
    "    'Dublin':4970000, \n",
    "    'Oslo':4868681, \n",
    "    'Amsterdam':8000000, \n",
    "    'Copenhagen':1630000, \n",
    "    'Stockholm':2080000, \n",
    "    'Vienna':6690000, \n",
    "    'Helsinki':4000000, \n",
    "    'Berlin':4940000, \n",
    "    'Hamburg':1450000, \n",
    "    'Munich':5250000, \n",
    "    'Brussels':2710000, \n",
    "    'London':19880000, \n",
    "    'Paris':18030000, \n",
    "    'Lyon':4868539, \n",
    "    'Milan':7650000, \n",
    "    'Rome':7120000, \n",
    "    'Madrid':5260000, \n",
    "    'Barcelona':8200000, \n",
    "    'Ljubljana':841320, \n",
    "    'Prague':5810000, \n",
    "    'Lisbon':3630000, \n",
    "    'Oporto':4868539, \n",
    "    'Athens':2680000, \n",
    "    'Bratislava':1400000, \n",
    "    'Budapest':3360000, \n",
    "    'Warsaw':1370000, \n",
    "    'Krakow':1313277, \n",
    "    'Edinburgh':1660000}\n",
    "\n",
    "    data['City_Turists'] = data['City'].replace(cities3)\n",
    "    \n",
    "    \n",
    "    # создаем признаки 'Rest_on_Pop' и 'Rest_on_Turists',\n",
    "    # которые содержыт информацию о количестве ресторанов на одного жителя и туриста\n",
    "    data['Rest_on_Pop'] = data['City_Num_rest']/data['City_Population']\n",
    "    data['Rest_on_Turists'] = data['City_Num_rest']/data['City_Turists']\n",
    "\n",
    "\n",
    "    # ##########      4.3 Cuisine_Style      ########## \n",
    "\n",
    "    # создаем признак 'Cuisine_Style_isNAN',который содержит инфо о пропусках\n",
    "    data['Cuisine_Style_isNAN'] = pd.isna(data['Cuisine_Style']).astype('uint8')\n",
    "\n",
    "    # Чистим данные от [] и '',\n",
    "    # приведем к одному формату (lower, без пробелов),\n",
    "    # преобразуем в список\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: str(x))\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\"[\",\"\"))\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\"]\",\"\"))\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\"'\",\"\"))\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.lower())\n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "    # Создаем словарь ключь - кухня, значение - количество ресторанов с этой кухней\n",
    "    cuisine_exploded = data.explode('Cuisine_Style')\n",
    "    cousins = dict(cuisine_exploded['Cuisine_Style'].value_counts())\n",
    "\n",
    "    # Создаем списки кухонь, по частоте встречаемости\n",
    "    all_hight_1000 = []\n",
    "    top_top = []\n",
    "    top_mid = []\n",
    "    top_low = []\n",
    "\n",
    "    for i, j in cousins.items():\n",
    "        if j > 10000:\n",
    "            all_hight_1000.append(i)\n",
    "            top_top.append(i)\n",
    "        elif j < 1000:\n",
    "            top_low.append(i)\n",
    "        else:\n",
    "            all_hight_1000.append(i)\n",
    "            top_mid.append(i)\n",
    "\n",
    "    # Создаем бинарную переменную наличия у ресторана кухни, с частотой встречаемости более 1 000 \n",
    "    for i in all_hight_1000:\n",
    "        fin = []\n",
    "        for j in range(len(data['Cuisine_Style'])):\n",
    "            if i in data['Cuisine_Style'][j]:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        data['More_1000'] = fin\n",
    "\n",
    "\n",
    "    # Создаем бинарную переменную наличия у ресторана кухни, с частотой встречаемости более 10 000 \n",
    "    for i in top_top:\n",
    "        fin = []\n",
    "        for j in range(len(data['Cuisine_Style'])):\n",
    "            if i in data['Cuisine_Style'][j]:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        data['Top_top'] = fin\n",
    "\n",
    "    # заполним пропуски наиболее часто встречающимся видом кухни \n",
    "    data['Cuisine_Style'] = data['Cuisine_Style'].apply(lambda x: 'vegetarianfriendly' if x[0] == 'nan' else x)\n",
    "\n",
    "    # создадим новый признак - количество предлагаемых в ресторане кухонь\n",
    "    data['Cuisines'] = data['Cuisine_Style'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Отнормируем количество предлагаемых кухонь по среднему значению по городу\n",
    "    data['Cuisines_norm'] = data['Cuisines']/data.City.map(dict(data.groupby(['City'])['Cuisines'].mean()))\n",
    "\n",
    "\n",
    "    # ##########      4.4 Price_Range      ########## \n",
    "    \n",
    "    # создаем признак, который содержит инфо о пропусках\n",
    "    data['Price_Range_isNAN'] = pd.isna(data['Price_Range']).astype('uint8')  \n",
    "  \n",
    "    # Создадим словарь для преобразования значений уровня цен\n",
    "    price_dict = {'$': 1, '$$ - $$$': 2, '$$$$': 3}\n",
    "    \n",
    "    # выполним преобразование\n",
    "    data['Price_Range'].replace(price_dict, inplace=True)\n",
    "    \n",
    "    # заполним пропущенные значения наиболее часто встречающимся\n",
    "    data['Price_Range'].fillna(data.City.map(dict(data.groupby(['City'])['Price_Range'].mean())), inplace=True)\n",
    "    data['Price_Range'] = data['Price_Range'].apply(round)\n",
    "\n",
    "    \n",
    "\n",
    "    # ##########      4.5 Number_of_Reviews      ########## \n",
    "\n",
    "    # создаем признак 'Number_of_Reviews_isNAN',который содержит инфо о пропусках\n",
    "    data['Number_of_Reviews_isNAN'] = pd.isna(data['Number_of_Reviews']).astype('uint8')    \n",
    "\n",
    "    # Заполним пропуски средним значением по городу   \n",
    "    data['Number_of_Reviews'].fillna(data.City.map(dict(data.groupby(['City'])['Number_of_Reviews'].mean())), inplace=True)\n",
    "    \n",
    "    # Создадим признак с информацией о среднем числе отзывов на одного туриста\n",
    "    data['Reviews_per_turist'] = data['Number_of_Reviews']/data['City_Turists']\n",
    "    \n",
    "    # Отнормируем количество отзывов по средне-городскому значению\n",
    "    data['Number_of_Reviews_norm'] = data['Number_of_Reviews']/data.City.map(dict(data.groupby(['City'])['Number_of_Reviews'].mean()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ##########      4.6 Reviews      ##########    \n",
    "    \n",
    "    # создаем новый признак 'Review_len' со средний длинной двух отзывов\n",
    "    data['Reviews_len'] = data['Reviews'].apply(lambda x: ((len(x[0])+len(x[1]))/2) if len(x) == 4 else len(x[0]))\n",
    "    \n",
    "    # Нормируем Review_len по среднему по городу\n",
    "    data['Reviews_len_norm'] = data['Reviews_len']/data.City.map(dict(data.groupby(['City'])['Reviews_len'].mean()))\n",
    "    \n",
    "    # Заполним пропуски средним значением\n",
    "    data['Reviews_len'].replace(to_replace=0, value=data['Reviews_len'].mean(), inplace=True)\n",
    "\n",
    "    # создаем новый признак 'Review_Delta' с разницей в днях между двумя отзывыами из массива:\n",
    "    data['Review_Delta'] = data['Review_1'] - data['Review_2']\n",
    "    data['Review_Delta'] = data['Review_Delta'].apply(lambda x: x.days)\n",
    "\n",
    "    # создаем новый признак 'Review_Delta_now' с разницей в днях между последним отзывом и сегодняшней датой:\n",
    "    data['Review_Delta_now'] = datetime.strptime('02/23/2021', '%m/%d/%Y') - data['Review_1'] \n",
    "    data['Review_Delta_now'] =  data['Review_Delta_now'].apply(lambda x: x.days)\n",
    "\n",
    "    # Создадим признак Reviews_repit, отражающий наличие повторных отзывов среди 2х представленных\n",
    "    #(повторы на уровне всей базы)\n",
    "    c = collections.Counter()\n",
    "\n",
    "    for i in range(len(data['Reviews'])):\n",
    "        if len(data['Reviews'][i]) == 4:\n",
    "            c[data['Reviews'][i][0]] += 1\n",
    "            c[data['Reviews'][i][1]] += 1\n",
    "        if len(data['Reviews'][i]) == 2 and len(data['Reviews'][i][0]) > 0:\n",
    "            c[data['Reviews'][i][0]] += 1\n",
    "\n",
    "    fin = []\n",
    "\n",
    "    for i in range(len(data['Reviews'])):\n",
    "        if len(data['Reviews'][i]) == 4:\n",
    "            if c[data['Reviews'][i][0]] > 1 or c[data['Reviews'][i][1]] > 1:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        elif len(data['Reviews'][i]) == 2 and len(data['Reviews'][i][0]) > 0:\n",
    "            if c[data['Reviews'][i][0]] > 1:\n",
    "                fin.append(1)\n",
    "            else:\n",
    "                fin.append(0)\n",
    "        else:\n",
    "            fin.append(0)\n",
    "\n",
    "    data['Reviews_repit'] = fin\n",
    "    \n",
    "    # ##########      4.7 Ranking      ##########    \n",
    "    \n",
    "    # Нормируем Ranking по среднему по городу \n",
    "    data['Ranking_norm'] = data['Ranking']/data.City.map(dict(data.groupby(['City'])['Ranking'].mean()))\n",
    "\n",
    "    # ################### 5. Clean #################################################### \n",
    "    # убираем признаки которые еще не успели обработать, \n",
    "    # модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n",
    "    object_columns = [s for s in data.columns if data[s].dtypes == 'object' or data[s].dtypes == 'datetime64[ns]']\n",
    "    data.drop(object_columns, axis = 1, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запускаем и проверяем что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc = preproc_data(data)\n",
    "df_preproc.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь выделим тестовую часть\n",
    "train_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \n",
    "Это поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "Сам ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки:\n",
    "from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\n",
    "from sklearn import metrics # инструменты для оценки точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\n",
    "model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-913e4066c824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Обучаем модель на тестовом наборе данных\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Предсказанные значения записываем в переменную y_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Обучаем модель на тестовом наборе данных\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n",
    "# Предсказанные значения записываем в переменную y_pred\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n",
    "# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Если все устраевает - готовим Submission на кагл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Rating'] = predict_submission\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "Или что делать, чтоб улучшить результат:\n",
    "* Обработать оставшиеся признаки в понятный для машины формат\n",
    "* Посмотреть, что еще можно извлечь из признаков\n",
    "* Сгенерировать новые признаки\n",
    "* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n",
    "* Подобрать состав признаков\n",
    "\n",
    "В общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
